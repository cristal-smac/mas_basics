{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La théorie du bien-être social\n",
    "\n",
    "Auteurs : Philippe Mathieu et Antoine Nongaillard, [CRISTAL Lab](https://www.cristal.univ-lille.fr/), [SMAC Team](https://www.cristal.univ-lille.fr/?rubrique26&id=7), [University of Lille](http://www.univ-lille1.fr), email : philippe.mathieu@univ-lille.fr\n",
    "\n",
    "Creation : 18/12/2021\n",
    "\n",
    "\n",
    "## Principe\n",
    "\n",
    "Le problème présenté ici, connu sous l'acronyme **MARA** (MultiAgent Resource Allocation), rentre dans le cadre de l'affectation de ressources à un certain nombre d'agents. On considère un ensemble de ressources et un ensemble d'agents à qui l'on doit distribuer ces ressources. Les ressources considérées ici sont discrètes, indivisibles et non-partageables. Chaque agent étant autonome, il n'accorde pas la même importance aux différentes ressources que ses congénères : il exprime ses préférences au moyen d'une valeur associée à chaque ressource. Pour mesurer sa satisfaction individuelle on utilise ici une fonction d'utilité qui, pour chaque ressource, indique la valeur que l'agent lui accorde. L'agrégation des différentes valeurs associées aux ressources que possède un agent détermine sa satisfaction individuelle. D'autres considérations pourraient bien-sûr être prises en compte, nous restons ici dans un cadre simple. Le bien-être individuel d'un agent est évalué par la somme des utilités qu'il accorde aux ressources qu'il possède (on dit que les fonctions d'utilité sont additives).\n",
    "\n",
    "La collectivité doit aussi s'accorder sur une mesure de bien-être. Pour mesurer le bien-être social, il existe 4 grandes méthodes classiques:\n",
    "\n",
    "- Utilitaire : maximiser la somme des utilités des agents\n",
    "- Egalitaire : maximiser le minimum  des utilités des agents\n",
    "- Elitiste : maximiser le maximum  des utilités des agents\n",
    "- Nash : maximiser le produit des utilités des agents\n",
    "\n",
    "Le problème consiste alors à trouver comment affecter l'ensemble des ressources aux différents agents de manière à maximiser le bien-être social choisi.\n",
    "\n",
    "Cette feuille montre comment, à l'aide d'une approche comportementale, il est possible de s'approcher voire de calculer l'optimum social.\n",
    "\n",
    "### Comment représenter ressources et fonctions d'utilité\n",
    "\n",
    "Afin de pouvoir jouer sur les nombres de ressources et/ou d'agents, nous générerons aléatoirement des noms de ressources afin de pouvoir en avoir autant que l'on souhaite.  Pour la fonction d'utilité, on utilisera un dictionnaire Python, dont les clés sont les ressources potentielles et les valeurs le poids que l'agent y accorde. Pour ces valeurs on prendra un entier naturel (donc sans le zero, qui évidemment pose problème lors d'un produit de Nash). Par convention, plus une ressource sera importante aux yeux d'un agent, plus la valeur qu'il lui associera sera grande.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd # mainly for results\n",
    "import random       # mainly for choice et randrange\n",
    "# import string     # mainly for ascii_uppercase\n",
    "import functools    # mainly for reduce\n",
    "import copy         # mainly for deepcopy \n",
    "\n",
    "nb_resources =10                        # nb de ressources souhaités\n",
    "valeurs = list(range(1,20))  # intervalle de valuation de chaque ressource\n",
    "\n",
    "def genere_nom():\n",
    "    return ''.join(random.choice('ABCDEFGHIJKLMNOPQRSTUVWXYZ') for _ in range(5))\n",
    "   \n",
    "# Generer une liste de ressources\n",
    "ressources = [genere_nom() for _ in range(nb_resources)]\n",
    "print(\"Un ensemble de ressources \\t\", ressources)\n",
    "\n",
    "# Creer une fonction d'utilité avec des valeurs aléatoires\n",
    "u = {r : random.choice(valeurs) for r in ressources}\n",
    "print(\"Une fonction d'utilité\\t\" , u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On passe aux agents\n",
    "Un agent est défini avec le lot (`bag`) de ressources qu'il possède et une fonction d'utilité `fUtil` lui permettant de l'évaluer.\n",
    "Lors de sa création, la fonction d'évalution de l'agent est définie aléatoirement. Le lot est initialement vide. La méthode `welfare` permet de connaître le bien-être individuel de l'agent, ou en d'autres termes, comment l'agent évalue son lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,name) :\n",
    "        self.name=name\n",
    "        self.fUtil = {r : random.choice(valeurs) for r in ressources}\n",
    "        self.bag = []\n",
    "    def __str__(self) :\n",
    "        return \"agent \"+str(self.name)+\"  \"+ str(len(self.bag)) + \" resources. Individual welfare \"+ str(self.welfare())\n",
    "    def welfare(self):\n",
    "        return sum([self.fUtil.get(r) for r in self.bag]) \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques manipulations basiques sur un agent pour s'entrainer ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=Agent(\"ag\")\n",
    "a.bag=random.sample(ressources,5) # choices : avec remplacement ; sample : sans remplacement\n",
    "\n",
    "# le bag de l'agent a\n",
    "print(\"Bag \\t\\t\", a.bag)                              # ou sorted(a.bag)\n",
    "\n",
    "# la fonction d'utilité de l'agent a\n",
    "print(\"fUtil \\t\\t\", a.fUtil)                        # ou dict(sorted(a.fUtil.items()))\n",
    "\n",
    "# le bien-être de l'agent a\n",
    "print(\"welfare \\t\", a.welfare())\n",
    "\n",
    "# le bag avec ses valeurs\n",
    "bv = [ (r,a.fUtil.get(r)) for r in a.bag]\n",
    "print(\"valued bag \\t\" , bv )\n",
    "\n",
    "# le bag valué , trié par ordre croissant des valeurs\n",
    "print(\"valued and sorted bag \\t\", sorted(bv, key=lambda x: x[1]) )\n",
    "\n",
    "# les ressources, triées par valuation (la premier est celle dont on se débarrasse le plus facilement\n",
    "print(\"sorted resources\\t\",  [r for r,_ in sorted(bv, key=lambda x: x[1])]    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant créer autant d'agents que l'on souhaite, leur affecter des ressources et calculer leur bien-être individuel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creer 5 agents\n",
    "agentList = [Agent(\"ag\"+str(i)) for i in range(5) ]\n",
    " \n",
    "# distribuer les ressources aléatoirement\n",
    "for i in range(len(ressources)) :\n",
    "    random.choice(agentList).bag.append(ressources[i])\n",
    "\n",
    "# afficher les agents\n",
    "for a in agentList: print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approche centralisée\n",
    "Maintenant que les agents ont un `bag` rempli, il est maintenant possible de calculer le bien-être de cette société d'agents.\n",
    "Le calcul du bien-être social consiste à agréger (selon la fonction de bien-être souhaitée) les bien-êtres individuels de chaque agent. Par exemple, le bien-être-social utilitaire correspond à la somme des bien-êtres individuels.\n",
    "\n",
    "| nom | fn à maximiser | Caractéristique |\n",
    "| -----| ----|--- |\n",
    "| utilitaire | somme | équilibre global, sans tenir compte des bien-être individuels |\n",
    "| égalitaire | min | permet de satisfaire un minimum de désir pour tous les agents \n",
    "| élististe | max | on donne tout à son \"champion\" |\n",
    "| Nash | produit | réduire les écarts entre les individus, diminution des inégalités |\n",
    "\n",
    "Notons par ailleurs que la comparaison des valeurs atteintes par ces différentes fonctions de bien-être social n'a pas de sens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Utilitarian Social Welfare\\t', functools.reduce(lambda a, b: sum([a,b]) , [a.welfare() for a in agentList]) )\n",
    "print('Egalitarian Social Welfare\\t', functools.reduce(lambda a, b: min([a,b]) , [a.welfare() for a in agentList]) )\n",
    "print('Elitist Social Welfare\\t\\t',   functools.reduce(lambda a, b: max([a,b]) , [a.welfare() for a in agentList]) )\n",
    "print('Nash Social Welfare\\t\\t', functools.reduce(lambda a, b: np.prod([a,b]) , [a.welfare() for a in agentList]) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notion d'optimal social\n",
    "\n",
    "Parmi toutes les affectations possibles de différentes ressources, certaines fournissent la valeur maximale possible à la fonction de bien-être social. Pour certains bien-être sociaux cette affectation est facile à calculer (bien-être élitiste ou utilitaire) , pour d'autres c'est beaucoup plus compliqué. De plus, on considère ici qu'une \"main invisible\" (clin d'oeil à [Adam Smith](https://fr.wikipedia.org/wiki/Main_invisible)) distribue les ressources comme par magie. Si on souhaite prendre en compte les relations sociales entre les agents, cela devient compliqué même pour les fonctions d'utilité les plus simples.\n",
    "\n",
    "##### Solution optimale pour le bien-être social élitiste\n",
    "Le bien-être social **élitiste** consiste à **maximiser la richesse du plus riche** des agents (tout donner à son \"champion\" en quelque sorte). Quand les utilités sont 1-additives, calculer cet optimal est facile puisqu'il suffit de donner toutes les ressources à l'agents qui valorise le mieux la totalité des ressources.\n",
    "La \"main invisible\" affecte tour à tour tous les produis à chaque agent, et regarde qui est au bien-être maximum (on pourrait imaginer faire la somme des valuations de la fonction d'utilité sans affecter les ressources, mais cela ne fonctionnerait que si une ressource n'apparaît qu'une et une seule fois)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on affecte toutes les ressources à tous les agents\n",
    "for a in agentList:\n",
    "    a.bag = ressources.copy()\n",
    "\n",
    "# On recherche la plus forte évaluation\n",
    "index = np.argmax([a.welfare() for a in agentList])\n",
    "m = agentList[index].welfare()\n",
    "\n",
    "print(\"Elitist. Optimal value : \", m,\". Obtained by giving all the resources to agent \", agentList[index].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution optimale pour le bien-être social utilitaire\n",
    "Le bien-être social **utilitaire** consiste à **maximiser la somme** des richesses de tous les agents. Calculer cet optimal est facile puisqu'il suffit de donner chaque ressource à l'agent qui la valorise le mieux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on réinitialise les bags\n",
    "for a in agentList :\n",
    "        a.bag=[]\n",
    "\n",
    "# on affecte les ressources 1 par 1 à l'agent qui la valorise le mieux               \n",
    "for r in ressources :\n",
    "        index = np.argmax([a.fUtil.get(r)  for a in agentList])\n",
    "        ag = agentList[index]\n",
    "        ag.bag.append(r)\n",
    "\n",
    "# On fait la somme des évaluation\n",
    "s = sum([a.welfare() for a in agentList])\n",
    "\n",
    "print(\"Utilitarist. Optimal value : \", s ,\" obtained using the following affectations :\")\n",
    "for a in agentList :\n",
    "        print(a, a.bag)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution optimale pour les bien-êtres égalitaires ou Nash\n",
    "Cette fois il n'y a pas de solution simple, même pour une \"main invisible\". Il faudrait tester toutes les allocations possibles. Avec `n` agents et `r` ressources il existe `n^r` affectations possibles (si on a 10 agents et 100 ressources cela fait 10^100 tests, ce qui est hors de portée des ordinateurs actuels).\n",
    "On peut alors éventuellement utiliser des méthodes approximatives type _Monte-Carlo_ ou _Algorithme génétique_, mais rien ne garantit l'optimalité du résultat.\n",
    "\n",
    "Ci-dessous une méthode de _Monte-Carlo_ pour le bien-être de Nash ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestWelfare=-1\n",
    "bestAlloc=[]\n",
    "for i in range(100000):\n",
    "    # on réinitialise les bags\n",
    "    for a in agentList :\n",
    "        a.bag=[]\n",
    "    # on affecte les ressources au hasard\n",
    "    for r in ressources :\n",
    "        a = random.choice(agentList)\n",
    "        a.bag.append(r)\n",
    "    # On compare le résultat à la meilleure situation connue\n",
    "    p = np.prod([a.welfare() for a in agentList])  # sum,prod,min,max\n",
    "    if (p > bestWelfare):\n",
    "        bestWelfare=p\n",
    "        bestAlloc = {a.name:a.bag for a in agentList}\n",
    "        \n",
    "print(\"Best allocation\",bestWelfare,bestAlloc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... Une amélioration, basée sur les permutations de ressources, est possible puisque l'on sait dès le départ que pour Nash il est préférable d'équilibrer les ressources entre les agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestWelfare=-1\n",
    "bestAlloc=[]\n",
    "for i in range(100000):\n",
    "    # on réinitialise les bags\n",
    "    for a in agentList :\n",
    "        a.bag=[]\n",
    "    # on découpe les ressources en paquets égaux\n",
    "    random.shuffle(ressources)\n",
    "    distrib = np.array_split(ressources, len(agentList))\n",
    "    for i in range(len(agentList)) :\n",
    "        agentList[i].bag = distrib[i]\n",
    "    # On compare le résultat à la meilleure situation connue\n",
    "    p = np.prod([a.welfare() for a in agentList])  # sum,prod,min,max\n",
    "    if (p > bestWelfare):\n",
    "        bestWelfare=p\n",
    "        bestAlloc = {a.name:a.bag for a in agentList}\n",
    "        \n",
    "print(\"Best allocation\",bestWelfare,bestAlloc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On note ici la différence entre \"Individual-based system\" and \"agent-based system\". Jusqu'à présent il y avait bien des agents, chacun avec ses propres connaissances, mais aucun comportement. Il faut la \"main invisible\" pour résoudre le problème. La majorité du travail algorithmique se fait hors des agents. L'approche \"agent-based\" est une approche comportementale: tout se passe dans le comportement de l'agent, c'est l'agent qui décide ! Il n'y a plus de \"main invisible\". Seul un SMA, qui n'est là que pour le strict minimum : donner la parole aux agents.\n",
    "\n",
    "\n",
    "## L'approche multi-agents\n",
    "\n",
    "L'approche centralisée présente un certain nombre de défauts :\n",
    "- Avoir une affectation optimale par cette technique n'est pas toujours possible (voir ci-dessus). \n",
    "- Cette technique oblige tous les agents à indiquer à l'organe central de calcul toutes leurs utilités. La garantie d'une certaine privacité est pourtant parfois nécessaire\n",
    "- La \"main invisible\" ! L'approche centralisée prend difficilement en compte le fait que les agents ne se connaissent pas forcément tous. Dans la vraie vie, les connaissances des individus dessinent un graphe social\n",
    "\n",
    "Par opposition, l'approche multi-agents possède plusieurs avantages :\n",
    "- Une approche multi-agents peut être implémentée avec une programmation distribuée. Dans ce cas les communications sont asynchrones par envoi de message avec accusés de réception et dead line pour la robustesse. C'est via ces communications qu'un agent peut être informé de l'état du système ou plus particulièrement de l'état de ses accointances.\n",
    "- Dans une exécution distribuée, des échanges de ressources peuvent avoir lieu de manière concurrente entre différents couples d'agents.\"\n",
    "- L'approche distribuée est évidemment bien plus conforme aux situations réelles.\n",
    "\n",
    "\n",
    "Nous étudions par la suite une approche comportementale qui permet aux agents d'échanger des ressources pour améliorer le bien-être social. Chaque agent tour à tour peut effectuer certains échanges avec ces accointances fidèles à un type de réseau social. On peut alors voir peu à peu le bien-être social évoluer. L'important maintenant, n'est plus \"quel est l'optimum ?\", mais \"comment on arrive à l'optimum ?\".\n",
    "\n",
    "\n",
    "##### Tout d'abord réalisons le SMA, le réseau social et l'agent ... \n",
    "- Cette fois, c'est le SMA qui crée les agents, dans son constructeur.\n",
    "- Les agents sont initialement créés vides.\n",
    "- Des méthodes permettent d'affecter aux agents des accointances ou des ressources. Ceci permettra pas la suite de relancer le même SMA soit avec la même affectation de ressources, soit avec le même réseau social, soit les deux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Le réseau social\n",
    "\n",
    "Comme dans [mas_basics]() on utilisera la librairie [networkx](https://networkx.org/) qui contient de nombreuses fonctions de création de graphes et permet de récupérer avec sa fonction `to_numpy_array` une matrice d'adjacence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "nb_nodes = 20\n",
    "graph = nx.barabasi_albert_graph(nb_nodes, 2)\n",
    "#graph = nx.complete_graph(nb_nodes)\n",
    "#graph = nx.erdos_renyi_graph(nb_nodes,0.6)\n",
    "#graph = nx.cycle_graph(nb_nodes)\n",
    "#graph = nx.random_regular_graph(3,nb_nodes)\n",
    "matrix = nx.to_numpy_array(graph, nodelist=range(nb_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et pour la représentation graphique ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,5))    # pour définir la taille de la figure\n",
    "nx.draw_networkx(graph)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L'agent et le SMA\n",
    "Pour créer un SMA on lui passe simplement le nombre d'agents souhaité. C'est le SMA qui crée les agents. La méthode `setResources` permet d'affecter les ressources aux agents (via une liste de listes décrivant les `bag`des agents). La méthode `setAccointances` établit la structure du réseau social via une matrice d'adjacence (sous forme d'une liste de listes obtenue facilement à partir d'un graphe networkx).\n",
    "L'agent est initialement créé sans accointance ni ressource, mais il a déjà sa fonction d'utilité.\n",
    "Attention : dans le code suivant, recréer un SMA recrée aussi les fonctions d'utilité de chacun !\n",
    "\n",
    "De nombreux comportements peuvent être mis en place. Sur le nombre de participants à un échange (on en reste ici à 2 : négociation bi-latérale), sur le type d'échange autorisé (échange d'une ressource contre une autre (swap) ou don d'une ressource sans contre-partie (gift)), ou sur l'ordre dans lequel on propose ces échanges. Même en se focalisant sur le swap il y a encore 3 grandes familles (`swap_kind`):\n",
    "- les irrationnels : ils échangent n'importe quoi avec n'importe qui, sans rien vérifier\n",
    "- les rationnels : l'échange ne se fait que si tous les deux y gagnent individuellement. Le don n'a pas de sens pour eux.\n",
    "- les sociaux : l'échange ne se fait que si la société y gagne. Ce qui se produit pour certains dons, mais pas pour tous !\n",
    "\n",
    "Par ailleurs, dans les trois cas précédents il est encore possible de s'y prendre de 2 manières différentes (`behavior_kind`)\n",
    "- Priorité à la ressource : on prend sa pire ressource et on cherche un ami avec qui on fera l'échange ; si on a fait le tour de ses amis sans succès, alors seulement on change de ressource. \n",
    "- Priorité à l'ami : on choisit un ami et on cherche une ressource à lui échanger ; si on a fait le tour de toutes les ressources sans succès avec cet ami, alors seulement on change d'ami.\n",
    "\n",
    "\n",
    "Dans l'implémentation qui suit et pour des raisons de simplification, on fait appel à l'objet `sma` pour connaître le bien être social,\n",
    "ce qui est contradictoire avec la décentralisation du processus.\n",
    "Notons que pour le calcul du bien être social, connaître la variation des utilités des 2 agents qui s'échangent des ressources\n",
    "est parfois suffisant pour en déduire que le bien être social (donc de l'ensemble du système) est amélioré ou pas.\n",
    "Ceci fait l'objet de l'exercice 4.\n",
    "\n",
    "TODO: Structuration du comportement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMA:\n",
    "    def __init__(self, nb_agents, welfare, behavior_kind , swap_kind):\n",
    "        self.agentList = [Agent(i, self, behavior_kind, swap_kind) for i in range(nb_agents)]\n",
    "        self.welfare_type=welfare\n",
    "        self.tick=0\n",
    "        self.history=[]\n",
    "\n",
    "    def __str__(self) :\n",
    "        s=\"\"\n",
    "        for i in range(len(self.agentList)):\n",
    "            s=s+(self.agentList[i].__str__()+\"\\n\")\n",
    "        s=s+(\"Social Welfare : \" + str(self.socialWelfare())) \n",
    "        return(s)\n",
    "\n",
    "    def setRessources(self,init_affect) :\n",
    "        if (len(init_affect) != len(self.agentList)) :\n",
    "            raise ValueError(\"Problem of size in setRessources :\", len(init_affect), len(agentList))\n",
    "        for i in range(len(self.agentList)) :\n",
    "            self.agentList[i].bag=init_affect[i]\n",
    "\n",
    "    def setAccointances(self,adjacency_matrix) :\n",
    "        if (len(adjacency_matrix[0]) != len(self.agentList)) :\n",
    "            raise ValueError(\"Problem of size in setAccointances :\", len(adjacency_matrix[0]) , len(agentList))\n",
    "        for i in range(len(self.agentList)) :\n",
    "            self.agentList[i].accointances = np.nonzero(adjacency_matrix[i])[0]\n",
    "\n",
    "    def run(self, rounds):\n",
    "        self.tick=0\n",
    "        self.history=[]\n",
    "        self.history.append(self.socialWelfare())\n",
    "        for i in range(0,rounds):\n",
    "            self.runOnce()\n",
    "    \n",
    "    def runOnce(self):\n",
    "        self.tick += 1\n",
    "        for agent in self.agentList:\n",
    "            agent.decide(self.tick)\n",
    "        print(\"tick \" + str(self.tick) + \" ended\")\n",
    "        self.history.append(self.socialWelfare())\n",
    "        print(\"Le Welfare actuel est de \", self.socialWelfare())\n",
    "\n",
    "    def socialWelfare(self):\n",
    "        if self.welfare_type.upper()=='UTILITARIST':\n",
    "            return sum([a.welfare() for a in self.agentList])\n",
    "        elif self.welfare_type.upper()=='EGALITARIST':\n",
    "            return min([a.welfare() for a in self.agentList])\n",
    "        elif self.welfare_type.upper()=='ELITIST':\n",
    "            return max([a.welfare() for a in self.agentList])\n",
    "        elif self.welfare_type.upper()=='NASH':\n",
    "            return np.prod([a.welfare() for a in self.agentList])\n",
    "        else :\n",
    "            raise ValueError(\"Unknown method in socialWelfare\")\n",
    "\n",
    "\n",
    "\n",
    "# L'agent est le même que précédemment excepté qu'il a maintenant ses propres accointances, sa fonction d'Utilité\n",
    "# et une méthode de décision avec différents échanges possibles\n",
    "# Par défaut il n'a ni ressources ni accointances\n",
    "class Agent:\n",
    "    def __init__(self, name, sma, behavior_kind, swap_kind) :\n",
    "        self.name=name\n",
    "        self.sma = sma\n",
    "        self.swap_kind = swap_kind\n",
    "        self.behavior_kind = behavior_kind\n",
    "        self.fUtil = {r : random.choice(valeurs) for r in ressources}\n",
    "        self.bag = []\n",
    "        self.accointances = []\n",
    "    def __str__(self) :\n",
    "        return \"agent \"+str(self.name)+\" Welfare :\"+ str(self.welfare())+\"\\tbag :\"+ str(self.bag)\n",
    "    def welfare(self):\n",
    "        return sum([self.fUtil.get(r) for r in self.bag])\n",
    "    def getSortedBag(self):\n",
    "        bagval = [ (r,self.fUtil.get(r)) for r in self.bag]\n",
    "        return [r for r,_ in sorted(bagval, key=lambda x: x[1])]\n",
    "\n",
    "    def gift(self):    \n",
    "        # choisir une accointance au hasard\n",
    "        friend = self.sma.agentList[np.random.choice(self.accointances)]\n",
    "        # choisir une de ses propres ressources au hasard\n",
    "        r = np.random.choice(self.bag)\n",
    "        # donner cette ressource à l'accointance\n",
    "        self.bag.remove(r)\n",
    "        friend.bag.append(r)\n",
    "        print(self.name, \"donne la ressource \",r,\" à l'agent \",friend.name)\n",
    "\n",
    "    # IRRATIONAL : swap sans aucune contrainte\n",
    "    def swap1(self):\n",
    "        # choisir une accointance au hasard\n",
    "        friend = self.sma.agentList[np.random.choice(self.accointances)]\n",
    "        # choisir une de ses propres ressources au hasard\n",
    "        r = np.random.choice(self.bag)\n",
    "        # choisir une des ressources de l'autre au hasard\n",
    "        if len(friend.bag)==0 :\n",
    "            return\n",
    "        s = np.random.choice(friend.bag)\n",
    "        # echanger les ressources\n",
    "        self.bag.remove(r)\n",
    "        friend.bag.remove(s)\n",
    "        friend.bag.append(r)\n",
    "        self.bag.append(s)\n",
    "        print(self.name, \"echange \",r,\" contre \",s, \" avec \",friend.name)\n",
    "    \n",
    "    # PRIORITE RESSOURCE : on choisit une ressource et on teste tous les amis avec\n",
    "    # trois cas possibles : swap_kind = irrational, rational, social\n",
    "    def swap2(self) :\n",
    "        # je propose mes ressources par intérêt décroissant (donc valeur croissante)\n",
    "        for r in self.getSortedBag() :\n",
    "            # à chacun de mes amis\n",
    "            for f in self.accointances :\n",
    "                friend = self.sma.agentList[f]\n",
    "                for s in  friend.bag :\n",
    "                    # on teste si c'est mieux pour les deux\n",
    "                    # UTILISER sma N'EST PAS TRES \"AGENT\" , NEANMOINS DES CRITERES LOCAUX EXISTENT\n",
    "                    actualSocialWelfare = self.sma.socialWelfare()\n",
    "                    actualMyWelfare = self.welfare()\n",
    "                    actualFriendWelfare = friend.welfare()\n",
    "                    self.bag.remove(r)\n",
    "                    friend.bag.remove(s)\n",
    "                    friend.bag.append(r)\n",
    "                    self.bag.append(s)\n",
    "                    # For irrational (equiv to swap1 ; swap1 is a simplified version)\n",
    "                    if self.swap_kind.upper()=='IRRATIONAL' :\n",
    "                        return\n",
    "                    # For rational welfare\n",
    "                    if self.swap_kind.upper()=='RATIONAL' and self.welfare() > actualMyWelfare and friend.welfare() > actualFriendWelfare :\n",
    "                        print(self.name, \"echange \",r,\" contre \",s, \" avec \",friend.name)\n",
    "                        print(self.swap_kind +\" : Welfare passe de \",actualSocialWelfare,\" à \", self.sma.socialWelfare())\n",
    "                        #self.sma.history.append(self.sma.socialWelfare())\n",
    "                        return                       \n",
    "                    # For social welfare\n",
    "                    if self.swap_kind.upper()=='SOCIAL' and self.sma.socialWelfare() > actualSocialWelfare :\n",
    "                        print(self.name, \"echange \",r,\" contre \",s, \" avec \",friend.name)\n",
    "                        print(self.swap_kind + \" : Welfare passe de \",actualSocialWelfare,\" à \", self.sma.socialWelfare())\n",
    "                        #self.sma.history.append(self.sma.socialWelfare())\n",
    "                        return\n",
    "                    # Si les IF ne fonctionnent pas, on remet tout en place\n",
    "                    self.bag.remove(s)\n",
    "                    friend.bag.remove(r)\n",
    "                    friend.bag.append(s)\n",
    "                    self.bag.append(r)\n",
    "    \n",
    "    # PRIORITE FRIEND : on choisit un ami, et on teste toutes les ressources\n",
    "    # trois cas possibles : swap_kind = irrational, rational, social\n",
    "    def swap3(self) :\n",
    "        # je parcours mes amis\n",
    "        for f in self.accointances :\n",
    "            friend = self.sma.agentList[f]\n",
    "            # puis je propose mes ressources par intérêt décroissant (donc valeur croissante)\n",
    "            for r in self.getSortedBag() :\n",
    "                for s in  friend.bag :\n",
    "                    # on teste si c'est mieux pour les deux\n",
    "                    # UTILISER sma N'EST PAS TRES \"AGENT\" , NEANMOINS DES CRITERES LOCAUX EXISTENT\n",
    "                    actualSocialWelfare = self.sma.socialWelfare()\n",
    "                    actualMyWelfare = self.welfare()\n",
    "                    actualFriendWelfare = friend.welfare()\n",
    "                    self.bag.remove(r)\n",
    "                    friend.bag.remove(s)\n",
    "                    friend.bag.append(r)\n",
    "                    self.bag.append(s)\n",
    "                    # For irrational (equiv to swap1 ; swap1 is a simplified version)\n",
    "                    if self.swap_kind.upper()=='IRRATIONAL' :\n",
    "                        return\n",
    "                    # For rational welfare\n",
    "                    if self.swap_kind.upper()=='RATIONAL' and self.welfare() > actualMyWelfare and friend.welfare() > actualFriendWelfare :\n",
    "                        print(self.name, \"echange \",r,\" contre \",s, \" avec \",friend.name)\n",
    "                        print(self.swap_kind +\" : Welfare passe de \",actualSocialWelfare,\" à \", self.sma.socialWelfare())\n",
    "                        #self.sma.history.append(self.sma.socialWelfare())\n",
    "                        return                       \n",
    "                    # For social welfare\n",
    "                    if self.swap_kind.upper()=='SOCIAL' and self.sma.socialWelfare() > actualSocialWelfare :\n",
    "                        print(self.name, \"echange \",r,\" contre \",s, \" avec \",friend.name)\n",
    "                        print(self.swap_kind + \" : Welfare passe de \",actualSocialWelfare,\" à \", self.sma.socialWelfare())\n",
    "                        #self.sma.history.append(self.sma.socialWelfare())\n",
    "                        return\n",
    "                    # Si les IF ne fonctionnent pas, on remet tout en place\n",
    "                    self.bag.remove(s)\n",
    "                    friend.bag.remove(r)\n",
    "                    friend.bag.append(s)\n",
    "                    self.bag.append(r)\n",
    "\n",
    "    def decide(self,tick):\n",
    "        # pour faire qq chose il faut au moins avoir des ressources et des amis\n",
    "        if (len(self.accointances)==0 or len(self.bag)==0) :\n",
    "            return\n",
    "        # s'écrit avec match-case à partir de python 3.10\n",
    "        if   self.behavior_kind.upper()=='GIFT'       : self.gift()\n",
    "        elif self.swap_kind.upper()=='IRRATIONAL' : self.swap1()\n",
    "        elif self.behavior_kind.upper()=='RESSOURCE'  : self.swap2()\n",
    "        elif self.behavior_kind.upper()=='FRIEND'     : self.swap3()\n",
    "        else : \n",
    "            raise ValueError(\"Impossible behavior in decide\")\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Une expérience\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_agents=5\n",
    "# une affectation aléatoire\n",
    "l=[[] for i in range(nb_agents)]\n",
    "for r in ressources :\n",
    "    l[random.randrange(nb_agents)].append(r)\n",
    "\n",
    "# choix du graphe social\n",
    "graph = nx.complete_graph(nb_nodes)\n",
    "m = nx.to_numpy_array(graph, nodelist=range(nb_agents))\n",
    "\n",
    "# L'experience\n",
    "sma = SMA(nb_agents, welfare='Utilitarist', behavior_kind='friend', swap_kind='social')\n",
    "sma.setAccointances(m)\n",
    "sma.setRessources(l)\n",
    "print(sma)\n",
    "sma.run(10)\n",
    "\n",
    "print(\"Final Social Welfare : \", sma.socialWelfare())\n",
    "print(\"Best Social Welfare : \", max(sma.history))\n",
    "print(sma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracé de l'évolution du social welfare\n",
    "Une fois l'expérience réalisée, il est alors possible de tracer l'évolution du social Welfare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sma.history)\n",
    "plt.plot(sma.history, \"-o\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut bien sûr comparer les comportements en jouant sur les paramètres `behavior_kind` et/ou `swap_kind`. Afin de s'assurer que l'on utilise exactement les mêmes affectations d'amis et de ressources, on clone le sma à chaque tour.\n",
    "On prendra garde à la constitution des points de la courbe : si on loggue l'évolution du welfare à chaque changement (dans les méthodes `swap` par exemple), il n'y aura pas les mêmes valeurs collectées; pour assurer le même nombre de valeurs et donc des courbes comparables, on ne loggue qu'à chaque fin de tick (donc dans `runOnce`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_agents=5\n",
    "# une affectation aléatoire\n",
    "l=[[] for i in range(nb_agents)]\n",
    "for r in ressources :\n",
    "    l[random.randrange(nb_agents)].append(r)\n",
    "\n",
    "# choix du graphe social\n",
    "#graph = nx.erdos_renyi_graph(nb_agents,0.1)\n",
    "graph = nx.complete_graph(nb_nodes)\n",
    "m = nx.to_numpy_array(graph, nodelist=range(nb_agents))\n",
    "\n",
    "# L'experience\n",
    "sma = SMA(nb_agents, welfare='Utilitarist', behavior_kind='', swap_kind='')\n",
    "sma.setAccointances(m)\n",
    "sma.setRessources(l)\n",
    "data=pd.DataFrame()\n",
    "for behavior in ['friend','ressource'] :\n",
    "    for swap in ['irrational','rational','social'] :\n",
    "        # We work on a copy\n",
    "        sma2 = copy.deepcopy(sma)\n",
    "        for a in sma2.agentList:\n",
    "            a.behavior_kind =  behavior\n",
    "            a.swap_kind = swap\n",
    "            a.sma=sma2  \n",
    "        sma2.run(20)\n",
    "        data[behavior+'_'+swap]=sma2.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data)\n",
    "data.plot(figsize=(15,8), grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercices\n",
    "\n",
    "1. Dans le code précédent, le comportement est soit swap soit don. Or le don n'est jamais qu'un swap avec une ressource vide. Comment modifier le swap pour qu'il intègre le don ? Bien évidemment ce don n'a pas de sens pour un agent rationnel puisque si les utilités sont toutes positives, l'agent fait baisser son utilité en donnant.\n",
    "1. Comme le hasard intervient dans chaque expérience (distribution initiale des ressources, graphe s'il n'est pas complet), il est important de pouvoir répéter les mesures afin de moyenner les résultats. \n",
    "Tracer un tableau indiquant le nombre de victoires pour les 6 cas précédents répétés 100 fois. \n",
    "1. Précédemment, nous avons présenté 4 mesures de bien-être social. Que donne le Leximin ? Le leximin consiste à maximiser dans l'ordre lexicographique les vecteurs des utilités de chaque agent classées du min au max\n",
    "1. Dans le code du swap social, on teste l'augmentation de l'utilité sociale en utilisant la classe `sma`. A priori l'agent ne connait que ses accointances. Il ne devrait pas pouvoir accès à tous les agents, donc pas à la classe sma. On triche !. Comment mettre en place des critères locaux permettant de savoir si le bien être social augmente sans avoir accès à tout le monde ?\n",
    "1. le type de graphe\n",
    "    - Est-ce que des phénomènes spécifiques se produisent selon la forme du graphe ou le taux de connectivité moyen ?\n",
    "1. le type d'échange accepté\n",
    "    - Actuellement il n'y a que des échanges de taille 0 ou 1. Qu'est-ce que cela change d'accepter des échanges de 2 ou 3 ressources en même temps ?\n",
    "1. Les agents sont pour l'instant tous homogènes. Ils appliquent tous le même comportement. Qu'est-ce que ça donne si la population est hétérogène ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliographie\n",
    "- Moulin, H. (2004). Fair division and collective welfare. MIT press.\n",
    "- Chevaleyre Y, Endriss U, Lang J, Maudet N (2005) Negotiating over small bundles of resources. In: AAMAS’05, pp 296–302\n",
    "- Nongaillard, A., & Mathieu, P. (2014). Agent-based reallocation problem on social networks. Group Decision and Negotiation, 23(5), 1067-1083.\n",
    "- Delahaye, J. P., & Mathieu, P. (2009). La répartition idéale des biens existe-t-elle?. Pour la science, 381, 88-93.\n",
    "- Ferber J. (1995). Systèmes Multi-Agents, vers une intelligence collective. InterEditions. [en ligne ici](http://www.lirmm.fr/~ferber/publications/LesSMA_Ferber.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Leximin Social Welfare\\t\\t', sorted([a.welfare() for a in agentList]) )\n",
    " \n",
    "# 1 if a<b ; -1 if b<a ; 0 if a==b\n",
    "def leximin(a,b) :\n",
    "    for i in range(len(a)) :\n",
    "        if (a[i]>b[i]) :\n",
    "           return 1\n",
    "        elif (a[i]<b[i]) :\n",
    "           return -1\n",
    "    return 0\n",
    "\n",
    "x=np.array([1,2,3,4])\n",
    "y=np.array([1,1,3,4])\n",
    "leximin(x,y) "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
